{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:46:33.889667Z",
     "start_time": "2021-09-11T18:46:19.329690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools\n",
      "  Downloading setuptools-58.0.4-py3-none-any.whl (816 kB)\n",
      "Installing collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 50.3.1.post20201107\n",
      "    Uninstalling setuptools-50.3.1.post20201107:\n",
      "      Successfully uninstalled setuptools-50.3.1.post20201107\n",
      "Successfully installed setuptools-58.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:47:02.218385Z",
     "start_time": "2021-09-11T18:46:59.530466Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tables in c:\\users\\ilan\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: numpy>=1.9.3 in c:\\users\\ilan\\anaconda3\\lib\\site-packages (from tables) (1.19.2)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: numexpr>=2.6.2 in c:\\users\\ilan\\anaconda3\\lib\\site-packages (from tables) (2.7.1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:28:17.238515Z",
     "start_time": "2021-09-11T18:28:17.206053Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'tables'.  Use pip or conda to install tables.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-180b44180df4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHDFStore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TRAAAAW128F429D538.h5\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TheData'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'myfile.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"format is not a defined argument for HDFStore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m         \u001b[0mtables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tables\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomplib\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcomplib\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_complibs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, raise_on_missing, on_version)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraise_on_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'tables'.  Use pip or conda to install tables."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with pd.HDFStore(\"TRAAAAW128F429D538.h5\", 'r') as d:\n",
    "    df = d.get('TheData')\n",
    "    df.to_csv('myfile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:22:08.934782Z",
     "start_time": "2021-09-11T18:22:08.919826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['analysis', 'metadata', 'musicbrainz']>\n",
      "['bars_confidence', 'bars_start', 'beats_confidence', 'beats_start', 'sections_confidence', 'sections_start', 'segments_confidence', 'segments_loudness_max', 'segments_loudness_max_time', 'segments_loudness_start', 'segments_pitches', 'segments_start', 'segments_timbre', 'songs', 'tatums_confidence', 'tatums_start']\n",
      "<HDF5 group \"/analysis\" (16 members)>\n",
      "<HDF5 group \"/metadata\" (5 members)>\n",
      "<HDF5 group \"/musicbrainz\" (3 members)>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "filename = \"TRAAAAW128F429D538.h5\"\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])\n",
    "    print(data)\n",
    "    for key in ['analysis', 'metadata', 'musicbrainz']:\n",
    "        print(f[str(key)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T12:06:17.597052Z",
     "start_time": "2021-09-12T12:04:30.617421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of song files: 7620\n",
      "all artist names extracted in: 0:01:43.855111\n",
      "found 3836 unique artist names\n",
      "b'The Stooges'\n",
      "b'Will To Will'\n",
      "b'Lunasicc'\n",
      "b'POLINA'\n",
      "b'Bruce Katz Band'\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "no such table: songs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a218f1263467>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;31m# we query the database\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[0mall_artist_names_sqlite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: songs"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tutorial for the Million Song Dataset\n",
    "\n",
    "by Thierry Bertin-Mahieux (2011) Columbia University\n",
    "   tb2332@columbia.edu\n",
    "   Copyright 2011 T. Bertin-Mahieux, All Rights Reserved\n",
    "\n",
    "This tutorial will walk you through a quick experiment\n",
    "using the Million Song Dataset (MSD). We will actually be working\n",
    "on the 10K songs subset for speed issues, but the code should\n",
    "transpose seamlessly.\n",
    "\n",
    "In this tutorial, we do simple metadata analysis. We look at\n",
    "which artist has the most songs by iterating over the whole\n",
    "dataset and using an SQLite database.\n",
    "\n",
    "You need to have the MSD code downloaded from GITHUB.\n",
    "See the MSD website for details:\n",
    "http://labrosa.ee.columbia.edu/millionsong/\n",
    "\n",
    "If you have any questions regarding the dataset or this tutorial,\n",
    "please first take a look at the website. Send us an email\n",
    "if you haven't found the answer.\n",
    "\n",
    "Note: this tutorial is developed using Python 2.6\n",
    "      on an Ubuntu machine. PDF created using 'pyreport'.\n",
    "\"\"\"\n",
    "\n",
    "# usual imports\n",
    "import hdf5_getters as GETTERS\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import datetime\n",
    "import sqlite3\n",
    "import numpy as np  # get it at: http://numpy.scipy.org/\n",
    "# path to the Million Song Dataset subset (uncompressed)\n",
    "\n",
    "# CHANGE IT TO YOUR LOCAL CONFIGURATION\n",
    "msd_subset_path = 'C:\\\\Users\\\\Ilan\\\\Documents\\\\ITC\\\\11.Final_Project\\\\MillionSongSubset'\n",
    "msd_subset_data_path = os.path.join(msd_subset_path, 'data')\n",
    "msd_subset_addf_path = os.path.join(msd_subset_path, 'AdditionalFiles')\n",
    "assert os.path.isdir(msd_subset_path), 'wrong path'  # sanity check\n",
    "\n",
    "# path to the Million Song Dataset code\n",
    "# CHANGE IT TO YOUR LOCAL CONFIGURATION\n",
    "msd_code_path = os.getcwd()\n",
    "assert os.path.isdir(msd_code_path), 'wrong path'  # sanity check\n",
    "# we add some paths to python so we can import MSD code\n",
    "# Ubuntu: you can change the environment variable PYTHONPATH\n",
    "# in your .bashrc file so you do not have to type these lines\n",
    "sys.path.append(os.path.join(msd_code_path, 'PythonSrc'))\n",
    "\n",
    "# imports specific to the MSD\n",
    "\n",
    "# the following function simply gives us a nice string for\n",
    "# a time lag in seconds\n",
    "\n",
    "\n",
    "def strtimedelta(starttime, stoptime):\n",
    "    return str(datetime.timedelta(seconds=stoptime-starttime))\n",
    "\n",
    "# we define this very useful function to iterate the files\n",
    "\n",
    "\n",
    "def apply_to_all_files(basedir, func=lambda x: x, ext='.h5'):\n",
    "    \"\"\"\n",
    "    From a base directory, go through all subdirectories,\n",
    "    find all files with the given extension, apply the\n",
    "    given function 'func' to all of them.\n",
    "    If no 'func' is passed, we do nothing except counting.\n",
    "    INPUT\n",
    "       basedir  - base directory of the dataset\n",
    "       func     - function to apply to all filenames\n",
    "       ext      - extension, .h5 by default\n",
    "    RETURN\n",
    "       number of files\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    # iterate over all files in all subdirectories\n",
    "    for root, dirs, files in os.walk(basedir):\n",
    "        files = glob.glob(os.path.join(root, '*'+ext))\n",
    "        # count files\n",
    "        cnt += len(files)\n",
    "        # apply function to all files\n",
    "        for f in files:\n",
    "            func(f)\n",
    "    return cnt\n",
    "\n",
    "\n",
    "# we can now easily count the number of files in the dataset\n",
    "print('number of song files:', apply_to_all_files(msd_subset_data_path))\n",
    "\n",
    "# let's now get all artist names in a set(). One nice property:\n",
    "# if we enter many times the same artist, only one will be kept.\n",
    "all_artist_names = set()\n",
    "\n",
    "# we define the function to apply to all files\n",
    "\n",
    "\n",
    "def func_to_get_artist_name(filename):\n",
    "    \"\"\"\n",
    "    This function does 3 simple things:\n",
    "    - open the song file\n",
    "    - get artist ID and put it\n",
    "    - close the file\n",
    "    \"\"\"\n",
    "    h5 = GETTERS.open_h5_file_read(filename)\n",
    "    artist_name = GETTERS.get_artist_name(h5)\n",
    "    all_artist_names.add(artist_name)\n",
    "    h5.close()\n",
    "\n",
    "\n",
    "# let's apply the previous function to all files\n",
    "# we'll also measure how long it takes\n",
    "t1 = time.time()\n",
    "apply_to_all_files(msd_subset_data_path, func=func_to_get_artist_name)\n",
    "t2 = time.time()\n",
    "print('all artist names extracted in:', strtimedelta(t1, t2))\n",
    "\n",
    "\n",
    "# let's see some of the content of 'all_artist_names'\n",
    "print('found', len(all_artist_names), 'unique artist names')\n",
    "for k in range(5):\n",
    "    print(list(all_artist_names)[k])\n",
    "\n",
    "# this is too long, and the work of listing artist names has already\n",
    "# been done. Let's redo the same task using an SQLite database.\n",
    "# We connect to the provided database: track_metadata.db\n",
    "conn = sqlite3.connect(os.path.join(msd_subset_addf_path,'subset_track_metadata.db'))\n",
    "# we build the SQL query\n",
    "q = \"SELECT DISTINCT artist_name FROM songs\"\n",
    "# we query the database\n",
    "t1 = time.time()\n",
    "res = conn.execute(q)\n",
    "all_artist_names_sqlite = res.fetchall()\n",
    "t2 = time.time()\n",
    "print('all artist names extracted (SQLite) in:', strtimedelta(t1, t2))\n",
    "# we close the connection to the database\n",
    "conn.close()\n",
    "# let's see some of the content\n",
    "for k in range(5):\n",
    "    print(all_artist_names_sqlite[k][0])\n",
    "\n",
    "# now, let's find the artist that has the most songs in the dataset\n",
    "# what we want to work with is artist ID, not artist names. Some artists\n",
    "# have many names, usually because the song is \"featuring someone else\"\n",
    "conn = sqlite3.connect(os.path.join(msd_subset_addf_path,\n",
    "                                    'subset_track_metadata.db'))\n",
    "q = \"SELECT DISTINCT artist_id FROM songs\"\n",
    "res = conn.execute(q)\n",
    "all_artist_ids = map(lambda x: x[0], res.fetchall())\n",
    "conn.close()\n",
    "\n",
    "# The Echo Nest artist id look like:\n",
    "for k in range(4):\n",
    "    print(all_artist_ids[k])\n",
    "\n",
    "# let's count the songs from each of these artists.\n",
    "# We will do it first by iterating over the dataset.\n",
    "# we prepare a dictionary to count files\n",
    "files_per_artist = {}\n",
    "for aid in all_artist_ids:\n",
    "    files_per_artist[aid] = 0\n",
    "\n",
    "# we prepare the function to check artist id in each file\n",
    "\n",
    "\n",
    "def func_to_count_artist_id(filename):\n",
    "    \"\"\"\n",
    "    This function does 3 simple things:\n",
    "    - open the song file\n",
    "    - get artist ID and put it\n",
    "    - close the file\n",
    "    \"\"\"\n",
    "    h5 = GETTERS.open_h5_file_read(filename)\n",
    "    artist_id = GETTERS.get_artist_id(h5)\n",
    "    files_per_artist[artist_id] += 1\n",
    "    h5.close()\n",
    "\n",
    "\n",
    "# we apply this function to all files\n",
    "apply_to_all_files(msd_subset_data_path, func=func_to_count_artist_id)\n",
    "\n",
    "# the most popular artist (with the most songs) is:\n",
    "most_pop_aid = sorted(files_per_artist,\n",
    "                      key=files_per_artist.__getitem__,\n",
    "                      reverse=True)[0]\n",
    "print(most_pop_aid, 'has', files_per_artist[most_pop_aid], 'songs.')\n",
    "\n",
    "# of course, it is more fun to have the name(s) of this artist\n",
    "# let's get it using SQLite\n",
    "conn = sqlite3.connect(os.path.join(msd_subset_addf_path,\n",
    "                                    'subset_track_metadata.db'))\n",
    "q = \"SELECT DISTINCT artist_name FROM songs\"\n",
    "q += \" WHERE artist_id='\"+most_pop_aid+\"'\"\n",
    "res = conn.execute(q)\n",
    "pop_artist_names = map(lambda x: x[0], res.fetchall())\n",
    "conn.close()\n",
    "print('SQL query:', q)\n",
    "print('name(s) of the most popular artist:', pop_artist_names)\n",
    "\n",
    "# let's redo all this work in SQLite in a few seconds\n",
    "t1 = time.time()\n",
    "conn = sqlite3.connect(os.path.join(msd_subset_addf_path,\n",
    "                                    'subset_track_metadata.db'))\n",
    "q = \"SELECT DISTINCT artist_id,artist_name,Count(track_id) FROM songs\"\n",
    "q += \" GROUP BY artist_id\"\n",
    "res = conn.execute(q)\n",
    "pop_artists = res.fetchall()\n",
    "conn.close()\n",
    "t2 = time.time()\n",
    "print('found most popular artist in', strtimedelta(t1, t2))\n",
    "print(sorted(pop_artists, key=lambda x: x[2], reverse=True)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
